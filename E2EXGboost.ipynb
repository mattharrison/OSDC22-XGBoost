{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End XGBoost\n",
    "\n",
    "Â©2022 MetaSnake\n",
    "\n",
    "`@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.4 please\n",
    "!pip install -U yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "We will also use SHAP, xgbfir, openpyxl, hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "!pip install dtreeviz feature_engine pybaobabdt xgbfir shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import dtreeviz\n",
    "except ImportError:\n",
    "    print(\"No dtreeviz\")\n",
    "from feature_engine import encoding, imputation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    import pybaobabdt\n",
    "except ImportError:\n",
    "    print(\"No pybaobabdt\")\n",
    "\n",
    "from sklearn import base, compose, datasets, ensemble, \\\n",
    "    metrics, model_selection, pipeline, preprocessing, tree\n",
    "import xgboost as xgb\n",
    "import yellowbrick.model_selection as ms\n",
    "from yellowbrick import classifier\n",
    "\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "I'll be demoing with Titanic ðŸ¤”ðŸ˜‰\n",
    "Your lab will be with Kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X, titanic_y = datasets.fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "titanic_y = titanic_y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweakTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    def __init__(self, ycol=None):\n",
    "        self.ycol = ycol\n",
    "        self.y_val = None\n",
    "        \n",
    "    def transform(self, X):\n",
    "        df = (X\n",
    "             .drop(columns=['name', 'ticket', 'home.dest', 'boat', 'body', 'cabin'])\n",
    "            )\n",
    "        if self.ycol in df:\n",
    "            self.y_val = df[self.ycol]\n",
    "        return df\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "\n",
    "titanic_pl = pipeline.Pipeline([('tweak', TweakTransformer()),\n",
    "    ('cat_impute', imputation.CategoricalImputer(imputation_method='missing',\n",
    "            variables=['sex', 'embarked'])),\n",
    "    ('cat', encoding.OneHotEncoder(top_categories=5, drop_last=True, \n",
    "            variables=['sex', 'embarked'])),\n",
    "    ('num_impute', imputation.MeanMedianImputer(imputation_method='median',\n",
    "                                               variables=['age', 'fare']))\n",
    "                       ])\n",
    "\n",
    "titanic_X_train, titanic_X_test, titanic_y_train, titanic_y_test = model_selection.train_test_split(\n",
    "    titanic_X, titanic_y, random_state=42, stratify=titanic_y)\n",
    "titanic_pl.fit_transform(titanic_X_train, titanic_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/kaggle-survey-2018.zip'\n",
    "fin = urllib.request.urlopen(url)\n",
    "#fin = open('kaggle-survey-2018.zip', mode='rb')\n",
    "data = fin.read()\n",
    "with open('kaggle-survey-2018.zip', mode='wb') as fout:\n",
    "    fout.write(data)\n",
    "with zipfile.ZipFile('kaggle-survey-2018.zip') as z:\n",
    "    print(z.namelist())\n",
    "    kag = pd.read_csv(z.open('multipleChoiceResponses.csv'))\n",
    "    kag_questions = kag.iloc[0]\n",
    "    raw = kag.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn(ser, n=5, default='other'):\n",
    "    counts = ser.value_counts()\n",
    "    return ser.where(ser.isin(counts.index[:n]), default)\n",
    "\n",
    "def tweak_kag(df):\n",
    "    return (df\n",
    "        #.query('Q3.isin([\"United States of America\", \"China\", \"India\"]) '\\\n",
    "        #       'and Q6.isin([\"Data Scientist\", \"Software Engineer\"])')\n",
    "        .loc[df.Q3.isin([\"United States of America\", \"China\", \"India\"]) &\n",
    "             df.Q6.isin([\"Data Scientist\", \"Software Engineer\"])]\n",
    "        .pipe(lambda df_:\n",
    "            df_.assign(**(df_.Q1.pipe(pd.get_dummies, drop_first=True, prefix='gender')),\n",
    "                       age=df_.Q2.str.slice(0,2).astype(int),\n",
    "                       **(df_.Q3.pipe(pd.get_dummies, drop_first=True, prefix='country')),\n",
    "                       education=df_.Q4.replace({'Masterâ€™s degree': 18,\n",
    "                         'Bachelorâ€™s degree': 16,\n",
    "                         'Doctoral degree': 20,\n",
    "                         'Some college/university study without earning a bachelorâ€™s degree': 13,\n",
    "                         'Professional degree': 19,\n",
    "                         'I prefer not to answer': None,\n",
    "                         'No formal education past high school': 12}),\n",
    "                       **(df_.Q5\n",
    "                              .pipe(topn, n=3)\n",
    "                              .replace({\n",
    "                        'Computer science (software engineering, etc.)': 'cs',\n",
    "                        'Engineering (non-computer focused)': 'eng',\n",
    "                        'Mathematics or statistics': 'stat'})\n",
    "                              .pipe(pd.get_dummies, drop_first=True, prefix='major')),\n",
    "                       title=df_.Q6,\n",
    "                       years_exp=(df_.Q8.str.replace('+','', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .astype(float)),\n",
    "                       compensation=(df_.Q9.str.replace('+','', regex=False)\n",
    "                           .str.replace(',','', regex=False)\n",
    "                           .str.replace('500000', '500', regex=False)\n",
    "                           .str.replace('I do not wish to disclose my approximate yearly compensation', '0', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .fillna(0)\n",
    "                           .astype(int)\n",
    "                           .mul(1_000)\n",
    "                                    ),\n",
    "                       python=df_.Q16_Part_1.fillna(0).replace('Python', 1),\n",
    "                       r=df_.Q16_Part_2.fillna(0).replace('R', 1),\n",
    "                       sql=df_.Q16_Part_3.fillna(0).replace('SQL', 1)\n",
    "               )#assign\n",
    "              \n",
    "        )#pipe\n",
    "        .rename(columns=lambda col:col.replace(' ', '_'))\n",
    "        .loc[:, 'gender_Male':]   \n",
    "        .dropna()\n",
    "       )\n",
    "kag = tweak_kag(raw)\n",
    "kag_X = kag.drop(columns='title')\n",
    "kag_y = (kag.title == 'Data Scientist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stumps, Trees, and Forests\n",
    "\n",
    "Decision trees use a greedy algorithm to split on a feature (column) that results in the most \"pure\" split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "stump = tree.DecisionTreeClassifier(max_depth=1)\n",
    "X_train = titanic_pl.fit_transform(titanic_X_train)\n",
    "X_test = titanic_pl.transform(titanic_X_test)\n",
    "stump.fit(X_train, titanic_y_train.astype(int))\n",
    "stump.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - died, 1 - survived\n",
    "stump.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broken but fingers crossed for future\n",
    "features = list(c for c in X_train.columns)\n",
    "#plt.rcParams[\"font.family\"] = \"Roboto\"\n",
    "pybaobabdt.drawTree(stump, size=10, dpi=72, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in X_train.columns)\n",
    "tree.plot_tree(stump, feature_names=features, filled=True, class_names=['Died', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit\n",
    "A stump is too simple. It has too much *bias*.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "* Add more features\n",
    "* Use a more complex model\n",
    "\n",
    "For a tree we can let it grow deeper which should do both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "A model is too complicated. It has too much variance.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "* Simplify or constrain (*regularize*)\n",
    "* Add more samples\n",
    "\n",
    "For a tree we can prune back the growth so that the leaf nodes are overly specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hi_variance = tree.DecisionTreeClassifier(max_depth=None)\n",
    "X_train = titanic_pl.fit_transform(titanic_X_train)\n",
    "hi_variance.fit(X_train, titanic_y_train)\n",
    "hi_variance.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in X_train.columns)\n",
    "_=tree.plot_tree(hi_variance, feature_names=features, filled=True, class_names=['Died', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit view to first 2\n",
    "features = list(c for c in X_train.columns)\n",
    "_=tree.plot_tree(hi_variance, feature_names=features, filled=True, \n",
    "                 class_names=['Died', 'Survived'],\n",
    "                max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tree Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* 'max_depth=None' - Tree depth\n",
    "* 'max_features=None' - Amount of features to examine for split\n",
    "* 'max_leaf_nodes=None' - Number of leafs\n",
    "* 'min_impurity_decrease=0' - Split when *impurity* is >= this value. (*Impurity* : 0 - 100% accurate, .3 - 70%. Going from 70% to 100% accurate is a decrease of .3) \n",
    "* 'min_samples_leaf=1', - Minimum samples at each leaf.\n",
    "* 'min_samples_split=2' - Minimum samples required to split a node.\n",
    "* 'min_weight_fraction_leaf=0' - The fraction fo the total weights required to be a leaf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(stump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Uses *bagging* to ensemble many trees in an attempt to lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, titanic_y_train)\n",
    "rf.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in X_train.columns)\n",
    "_=tree.plot_tree(rf.estimators_[0], feature_names=features, filled=True, class_names=['Died', 'Survived'],\n",
    "                max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* 'n_estimators=100' - Number of trees - should be *max_estimators*\n",
    "* 'oob_score=False' - Can estimate score when training (by using rows that weren't randomly selected). No need to hold out data\n",
    "* 'warm_start=False' - Can add more trees w/o starting over\n",
    "\n",
    "From tree:\n",
    "\n",
    "* 'max_depth=None' - Tree depth (1 to Infinity (`None`))\n",
    "* 'max_features=\"sqrt\"' - Amount of features to examine for split (1 to number of features (int). Float of percent (0. to 1.0). \"log2\" log2(n_features) or \"sqrt\"  sqrt(n_features). (Default square root number of features.)\n",
    "* 'max_leaf_nodes=None' - Number of leafs. Default (`None`) is unlimited.\n",
    "* 'min_impurity_decrease=0' - Split when *impurity* is >= this value. (0.0 to 1.0) (*Impurity* : 0 - 100% accurate, .3 - 70%) \n",
    "* 'min_samples_leaf=1', - Minimum samples at each leaf. (1 to n_samples).\n",
    "* 'min_samples_split=2' - Minimum samples required to split a node. (1 to n_samples)\n",
    "* 'min_weight_fraction_leaf=0' - The fraction (0.0 to 1.0) of the total weights required to be a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Show score estimated during training\n",
    "rf_oob = ensemble.RandomForestClassifier(random_state=42, oob_score=True)\n",
    "rf_oob.fit(X_train, titanic_y_train)\n",
    "rf_oob.score(X_test, titanic_y_test), rf_oob.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the .score result changes after raising n_estimators\n",
    "rf_ws = ensemble.RandomForestClassifier(random_state=42, oob_score=True, \n",
    "                                        warm_start=True, n_estimators=3)\n",
    "rf_ws.fit(X_train, titanic_y_train)\n",
    "print(rf_ws.score(X_test, titanic_y_test), rf_oob.oob_score_)\n",
    "\n",
    "# change parameter and call .fit again. Doesn't need to start over\n",
    "rf_ws.set_params(n_estimators=200)\n",
    "rf_ws.fit(X_train, titanic_y_train)\n",
    "rf_ws.score(X_test, titanic_y_test), rf_oob.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how changing n_estimators affects score\n",
    "results = []\n",
    "rf_ws = ensemble.RandomForestClassifier(random_state=42, warm_start=True, n_estimators=1)\n",
    "rf_ws.fit(X_train, titanic_y_train)\n",
    "for i in range(2,100):\n",
    "    rf_ws.set_params(n_estimators=i)\n",
    "    rf_ws.fit(X_train, titanic_y_train)\n",
    "    results.append(rf_ws.score(X_test, titanic_y_test))\n",
    "    #results.append(metrics.f1_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "    #results.append(metrics.precision_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "    #results.append(metrics.recall_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "    #results.append(metrics.roc_auc_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "pd.Series(results).plot(figsize=(8,4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Exercise\n",
    "\n",
    "* Create a decision tree for the Kaggle Data predicting whether title is data scientist or software engineer\n",
    "* What is the score?\n",
    "* Create a random forest for the Kaggle Data predicting whether title is data scientist or software engineer\n",
    "* What is the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Uses *boosting* to train a series of (weak) trees that try to correct the error of the previous output. (For classification this is mapped to a probability)\n",
    "\n",
    "Like golfing (you continue to putt or use a different club depending on first error). Decision tree would be a single tee off. Random forest would be averaging the tee offs. \n",
    "\n",
    "* Regularization\n",
    "* Parallel Processing\n",
    "* Missing Number Support\n",
    "* Category Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xg.fit(X_train, titanic_y_train.astype(int))\n",
    "xg.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's try w/ depth of 2 and 2 trees\n",
    "xg = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False, max_depth=2, n_estimators=2)\n",
    "xg.fit(X_train, titanic_y_train.astype(int))\n",
    "xg.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tree\n",
    "xgb.to_graphviz(xg, size='1,1', num_trees=0, fontsize='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second tree\n",
    "xgb.to_graphviz(xg, size='1,1', num_trees=1, fontsize='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go down the left path with\n",
    "# this data\n",
    "row = pd.Series({'pclass': 2.0,\n",
    " 'age': 28.0,\n",
    " 'sibsp': 8.0,\n",
    " 'parch': 2.0,\n",
    " 'fare': 69.55,\n",
    " 'sex_male': 0.0,\n",
    " 'sex_female': 1.0,\n",
    " 'sex_Missing': 0.0,\n",
    " 'embarked_S': 1.0,\n",
    " 'embarked_C': 0.0,\n",
    " 'embarked_Q': 0.0,\n",
    " 'embarked_Missing': 0.0}).to_frame().T\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result for survival = .7027\n",
    "# > .5 ... so Survives!\n",
    "# this is [prob death, prob survival]\n",
    "xg.predict_proba(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# sum up leafs and throw into \n",
    "# Example: male, class 3\n",
    "# .49 + .37\n",
    "\n",
    "vals = np.linspace(-10, 10)\n",
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "print(inv_logit(.49+.37))\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(vals, inv_logit(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Exercise\n",
    "\n",
    "* Create an XGBoost tree for the Kaggle Data predicting whether title is data scientist or software engineer\n",
    "* What is the score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "Because you can keep \"putting\" you can keep track of how far away you are from the hole and stop when you are closest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# defaults\n",
    "# 100 putts\n",
    "xg = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xg.fit(X_train, titanic_y_train.astype(int))\n",
    "xg.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "# Go up to 100 but stop after you haven't improved for 20 hits\n",
    "# Min value at round 10 validation_1-logloss:0.40012\n",
    "\n",
    "xg = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xg.fit(X_train, titanic_y_train.astype(int),\n",
    "       eval_set=[(X_train, titanic_y_train.astype(int)),\n",
    "                 (X_test, titanic_y_test.astype(int))],\n",
    "            early_stopping_rounds=20) \n",
    "xg.score(X_test, titanic_y_test.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the evaluation metrics\n",
    "# validation_0 is for training data\n",
    "# validation_1 is for testing data\n",
    "results = xg.evals_result()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the evaluation metrics\n",
    "results['validation_0']['logloss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score is best at 11 trees\n",
    "results = xg.evals_result()\n",
    "ax = pd.DataFrame({'training': results['validation_0']['logloss'],\n",
    "              'testing': results['validation_1']['logloss'],\n",
    "             }).shift().plot(figsize=(5,4))\n",
    "ax.set_xlabel('ntrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* Boosting\n",
    "\n",
    "  * ``n_estimators=100`` - number of trees (or boosting rounds). Larger is more complex. Default 100. Use ``early_stopping_rounds`` with ``.fit`` to prevent overfitting.\n",
    "\n",
    "  * ``learning_rate=.3`` (called ``eta`` too) - after each boosting step, shrink feature weights. Larger is more conservative. Can be used with n_estimators to adjust time for convergence [0,1], default .3\n",
    "\n",
    "  * ``gamma=0`` / ``min_split_loss`` - L0 regularization. Global regularization. Minimum loss required for split. Larger is more conservative. [0, âˆž], default 0 - No regularization.\n",
    "\n",
    "\n",
    "* Regularization\n",
    "\n",
    "  * ``reg_lambda=1`` - L2 regularization (Root of squared weights). Increase to be more conservative. Default 1\n",
    "  * ``reg_alpha=0`` - L1 regularization (Mean of weights). Increase to be more conservative. Default 0\n",
    "\n",
    "* Sampling - Use different rows\n",
    "\n",
    "  * ``subsample=1`` - Use % of samples (this is rows!) for next boosting round. Lower to more conservative. [0, 1], default 1. (When not equal to 1.0, model does *stochastic gradient descent*, ie. there is some randomness in the model.)\n",
    "\n",
    "\n",
    "New tree (sampling) parameters - Use different columns (not rows!):\n",
    "\n",
    "  * ``colsample_bytree=1`` - Fraction of columns for each boosting round.\n",
    "  \n",
    "  * ``colsample_bylevel=1`` - Fraction of columns for each depth level.\n",
    "  \n",
    "  * ``colsample_bynode=1`` - Fraction of columns for each node.\n",
    "  \n",
    "\n",
    "From tree:\n",
    "\n",
    "  * ``max_depth=6`` - depth of tree. Larger is more complex (more likely to overfit). How many feature interactions you can have. Each level doubles time. [0, âˆž], default 6\n",
    "  * ``min_child_weight=1`` - Stop splitting after certain amount of purity. Larger will be more conservative.\n",
    "\n",
    "\n",
    "Imbalanced data:\n",
    "\n",
    "* ``scale_pos_weight=1`` -  ratio negative/positive. Default 1\n",
    "* Use ``'auc'`` or ``'aucpr'`` for ``eval_metric`` metric (rather than classification default ``'logless'``)\n",
    "* ``max_delta_step=0`` - try values from 1-10. Default 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), X_train, titanic_y_train.astype(int),\n",
    "                    param_name='gamma', param_range=[0, .5, 1,2,5,10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(tree.DecisionTreeClassifier(), X_train, titanic_y_train.astype(int),\n",
    "                    param_name='max_depth', param_range=[1,2,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), X_train, titanic_y_train.astype(int),\n",
    "                    param_name='max_depth', param_range=[1,2,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this depends on n_estimators\n",
    "# should really use early stopping but yellowbrick doesn't support this ðŸ˜¢\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), X_train, titanic_y_train.astype(int),\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# this takes a while to run (about 2 minutes)\n",
    "# can set scoring in GridSearchCV to \n",
    "# recall, precision, f1, accuracy\n",
    "params = {'reg_lambda': [0],  # No effect\n",
    "          'learning_rate': [.1, .3], # makes each boost more conservative (0 - no shrinkage) \n",
    "          'subsample': [.7, 1],\n",
    "          #'gamma': [0, 1],\n",
    "          'max_depth': [2, 3],\n",
    "          'random_state': [42],\n",
    "          'n_jobs': [-1],\n",
    "          'n_estimators': [200]}\n",
    "xgb2 = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "cv = (model_selection.GridSearchCV(xgb2, params, cv=3, n_jobs=-1)\n",
    "    .fit(X_train, titanic_y_train.astype(int),\n",
    "         eval_set=[(X_test, titanic_y_test.astype(int))],\n",
    "         early_stopping_rounds=5) \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.1,\n",
    " 'max_depth': 3,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs default\n",
    "xgb_def = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_def.fit(X_train, titanic_y_train.astype(int))\n",
    "#xgb_grid = xgb.XGBClassifier(**cv.best_params_, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_grid = xgb.XGBClassifier(**params, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_grid.fit(X_train, titanic_y_train.astype(int))\n",
    "xgb_def.score(X_test, titanic_y_test.astype(int)), xgb_grid.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Exercise\n",
    "\n",
    "* Use grid search to evaluate hyperparameters for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Tuning with Hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our basic grid search example\n",
    "params = {'learning_rate': 0.1,\n",
    " 'max_depth': 3,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X_train = titanic_pl.fit_transform(titanic_X_train)\n",
    "X_test = titanic_pl.transform(titanic_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs default\n",
    "xgb_def = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_def.fit(X_train, titanic_y_train.astype(int))\n",
    "xgb_grid = xgb.XGBClassifier(**params, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_grid.fit(X_train, titanic_y_train.astype(int))\n",
    "xgb_def.score(X_test, titanic_y_test.astype(int)), xgb_grid.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score  \n",
    "#https://bradleyboehmke.github.io/xgboost_databricks_tuning/index.html#slide21\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, 0),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 12, 1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -2, 3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'gamma': hp.loguniform('gamma', -10, 10),\n",
    "    'reg_alpha': hp.loguniform('alpha', -10, 10),\n",
    "    'reg_lambda': hp.loguniform('lambda', -10, 10),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'seed': 123,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(space):    \n",
    "    model = xgb.XGBClassifier(max_depth = int(space['max_depth']), \n",
    "                gamma = space['gamma'],                                         \n",
    "                reg_alpha = int(space['reg_alpha']),\n",
    "                min_child_weight=space['min_child_weight'],                                 \n",
    "                colsample_bytree=space['colsample_bytree'])\n",
    "    evaluation = [(X_train, titanic_y_train.astype(int)),\n",
    "            (X_test, titanic_y_test.astype(int))]\n",
    "    model.fit(X_train, titanic_y_train.astype(int),\n",
    "                 eval_set=evaluation, eval_metric=\"rmse\",            \n",
    "                 early_stopping_rounds=10,verbose=False)    \n",
    "         \n",
    "    pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(titanic_y_test.astype(int), pred>0.5)    \n",
    "    print (\"SCORE:\", accuracy)    \n",
    "    #change the metric if you like    \n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning,            \n",
    "    space=space,           \n",
    "    algo=tpe.suggest,            \n",
    "    max_evals=1000,            \n",
    "    trials=trials,\n",
    "    #timeout=60*5 # 5 minutes\n",
    "           )\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best # new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train.astype(int)),\n",
    "            (X_test, titanic_y_test.astype(int))]\n",
    "xgb_hyp.fit(X_train, titanic_y_train.astype(int), early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test.astype(int))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp.score(X_test, titanic_y_test.astype(int))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs default and grid\n",
    "xgb_def.score(X_test, titanic_y_test.astype(int)), xgb_grid.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we've tuned our model, let's look at how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train.astype(int)),\n",
    "            (X_test, titanic_y_test.astype(int))]\n",
    "xgb_hyp.fit(X_train, titanic_y_train.astype(int), early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test.astype(int))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(titanic_y_test.astype(int), xgb_hyp.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.confusion_matrix(xgb_hyp, X_train, titanic_y_train.astype(int),\n",
    "                            X_test, titanic_y_test.astype(int),\n",
    "                            classes=['Died', 'Survived']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(xgb_hyp,\n",
    "                       X_test, titanic_y_test.astype(int),ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.precision_recall_curve(xgb_hyp, X_train, titanic_y_train.astype(int),\n",
    "                   X_test, titanic_y_test.astype(int),\n",
    "                   classes=['Died', 'Survived'],\n",
    "                   micro=False, macro=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.classification_report(xgb_hyp, X_train, titanic_y_train.astype(int),\n",
    "                   X_test, titanic_y_test.astype(int),\n",
    "                   classes=['Died', 'Survived'],\n",
    "                   micro=False, macro=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Exercise\n",
    "* Create a Confusion Matrix for your model\n",
    "* Create an ROCAUC curve \n",
    "* Create a PrecisionRecall curve\n",
    "* How is the model performing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training For Different Metrics\n",
    "\n",
    "We tuned our model. But we tuned it against accuracy. What if we want to optimize for recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy tuning\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), \n",
    "                    X_train, titanic_y_train.astype(int),\n",
    "    #                param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall tuning\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), \n",
    "                    X_train, titanic_y_train.astype(int),\n",
    "                    scoring='recall',\n",
    "                    #param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), \n",
    "                    X_train, titanic_y_train.astype(int),\n",
    "                    scoring='f1',\n",
    "                    param_name='max_depth', param_range=[1,2,5,10]\n",
    "#                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Trees are great when they overfit... They can explain what they overfit\n",
    "# (You can use these for \"surrogate models\")\n",
    "hi_variance = tree.DecisionTreeClassifier(max_depth=None)\n",
    "hi_variance.fit(X_train, titanic_y_train)\n",
    "hi_variance.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance shows the magnitude (not direction) of impact\n",
    "(pd.Series(hi_variance.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what happens\n",
    "dt3 = tree.DecisionTreeClassifier(max_depth=3)\n",
    "dt3.fit(X_train, titanic_y_train.astype(int))\n",
    "\n",
    "dtreeviz.trees.dtreeviz(dt3, X_train, titanic_y_train.astype(int), target_name='survivied',\n",
    "                       feature_names=X_train.columns, class_names=['died', 'survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost also supports feature importance\n",
    "xgb_def = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_def.fit(X_train, titanic_y_train.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(xgb_def.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * \"weight\" is the number of times a feature appears in a tree\n",
    "# * \"gain\" is the average gain of splits which use the feature\n",
    "# * \"cover\" is the average coverage of splits which use the feature\n",
    "xgb.plot_importance(xgb_def, importance_type='cover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the Model Exercise\n",
    "* What are the important features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgbfir (Feature Interactions Reshaped)\n",
    " *Gain*: Total gain of each feature or feature interaction\n",
    " \n",
    " *FScore*: Amount of possible splits taken on a feature or feature Interaction\n",
    " \n",
    " *wFScore*: Amount of possible splits taken on a feature or feature nteraction weighted by the probability of the splits to take place\n",
    " \n",
    " *Average wFScore*: wFScore divided by FScore\n",
    " \n",
    " *Average Gain*: Gain divided by FScore\n",
    " \n",
    " *Expected Gain*: Total gain of each feature or feature interaction weighted by the probability to gather the gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "xgbfir.saveXgbFI(xgb_def, feature_names=X_train.columns, OutputXlsxFile='fir.xlsx')\n",
    "pd.read_excel('fir.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interactions Exercise\n",
    "* What are the important features?\n",
    "* What are the second level interactions?\n",
    "* What are the third level interactions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP (SHapley Additive exPlantations)\n",
    "Should be *globally* consistent and accurate\n",
    "\n",
    " Shapley value (SHAP).\n",
    " \n",
    " From game theory, indicates how to distribute attribution of label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(xgb_def)\n",
    "vals = shap_ex.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explain an individual\n",
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_def.predict(X_test.iloc[[0]])  # predicts death... why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label is also death\n",
    "titanic_y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values show direction of feature impact\n",
    "# for this individual\n",
    "pd.Series(vals[0], index=X_test.columns).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the base value. We sum up the scores.\n",
    "# > 0 Positive Case\n",
    "shap_ex.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < 0 therefore ... Death\n",
    "shap_ex.expected_value + vals[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# use matplotlib if having js issues\n",
    "# blue - Dead\n",
    "# red - Survival\n",
    "shap.force_plot(shap_ex.expected_value, \n",
    "               vals[0,:], X_test.iloc[0], #matplotlib=True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a feature\n",
    "shap.dependence_plot('age', vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain another feature\n",
    "shap.dependence_plot('fare', vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a feature with an interaction\n",
    "shap.dependence_plot('fare', vals, X_test, interaction_index='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain global features\n",
    "shap.summary_plot(vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the Model Exercise\n",
    "* Use SHAP to explore predictions for a sample\n",
    "* Use SHAP to expore a feature\n",
    "* Use SHAP to explore the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other XGBoost Features\n",
    "\n",
    "* Need to when predicting use the tree limit\n",
    "* Monotonic constraints - Force relationship of column to target to be monotonic\n",
    "* Interaction constraints - Limit interactions between features.\n",
    "* Can simulate decision trees (just one estimator) and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Limit\n",
    "Best practice is to use `early_stopping_rounds` with `eval_set` in call to `.fit`.\n",
    "\n",
    "If you set `n_estimators` in the constructor (`xgb.XGBClassifier`) and set those, call to `.predict` and `.score` will only use `.best_ntree_limit` trees, not necessaritly `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train.astype(int)),\n",
    "            (X_test, titanic_y_test.astype(int))]\n",
    "xgb_hyp.fit(X_train, titanic_y_train.astype(int), early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test.astype(int))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score is not best at 2_000 trees\n",
    "results = xgb_hyp.evals_result()\n",
    "ax = pd.DataFrame({'training': results['validation_0']['logloss'],\n",
    "              'testing': results['validation_1']['logloss'],\n",
    "             }).plot(figsize=(5,4))\n",
    "ax.set_xlabel('ntrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp#.booster.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses best_ntree_limit\n",
    "xgb_hyp.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(titanic_y_test.astype(int),\n",
    "    xgb_hyp.predict(X_test, ntree_limit=xgb_hyp.best_ntree_limit)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all of the trees is not necessarily better\n",
    "metrics.accuracy_score(titanic_y_test.astype(int),\n",
    "    xgb_hyp.predict(X_test, ntree_limit=1521)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monotonic Constraints\n",
    "If you want to remove u or w shaped behavior in features, force a monotonic constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# no constraints\n",
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train.astype(int)),\n",
    "            (X_test, titanic_y_test.astype(int))]\n",
    "xgb_hyp.fit(X_train, titanic_y_train.astype(int), early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test.astype(int))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# add monotonic constraint to age\n",
    "# as age goes up survival goes down!\n",
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "# if age goes up survived (y) goes down (-1)\n",
    "constraints = [0 if col != 'age' else -1 \n",
    "              for col in X_train]\n",
    "cst = f'({\",\".join(map(str,constraints))})'\n",
    "# needs to be string '(1,0,0,-1,0,...)'\n",
    "xgb_hyp_age = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                                monotone_constraints=cst,\n",
    "                                use_label_encoder=False,\n",
    "                               n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train.astype(int)),\n",
    "            (X_test, titanic_y_test.astype(int))]\n",
    "xgb_hyp_age.fit(X_train, titanic_y_train.astype(int), early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp_age.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp_age.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our score with constraints is worse on accuracy\n",
    "xgb_hyp.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(xgb_hyp_age)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.dependence_plot('age', vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# no constrains\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(xgb_hyp)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.dependence_plot('age', vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Constraints\n",
    "You can limit what columns interact with other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like sex_male has an interaction with pclass (and age)\n",
    "fig, ax = plt.subplots(figsize=(20,30), dpi=300)\n",
    "xgb.plot_tree(xgb_hyp, rankdir='LR', num_trees=0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to specify index numbers of columns that can interact\n",
    "print(list(enumerate(X_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it so pclass and sex_male|sex_female can't interact\n",
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "# Needs to be string with nested list of index values\n",
    "interaction_constraints = str([[idx for idx, col in enumerate(X_train.columns)\n",
    "                                if col not in {'pclass', 'sex_male', 'sex_female'}]])\n",
    "# column names don't work (even though docs say they do)\n",
    "#interaction_constraints = str([[col for idx, col in enumerate(X_train.columns)\n",
    "#                                if col not in {'pclass', 'sex_male', 'sex_female'}]]).replace(\"'\", '\"')\n",
    "\n",
    "xgb_hyp_age = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                                interaction_constraints=interaction_constraints,\n",
    "                                use_label_encoder=False,\n",
    "                               n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train.astype(int)),\n",
    "            (X_test, titanic_y_test.astype(int))]\n",
    "xgb_hyp_age.fit(X_train, titanic_y_train.astype(int), early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp_age.score(X_test, titanic_y_test.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp_age.score(X_test, titanic_y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,30), dpi=300)\n",
    "xgb.plot_tree(xgb_hyp_age, rankdir='LR', num_trees=0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "xgbfir.saveXgbFI(xgb_hyp_age, feature_names=X_train.columns, OutputXlsxFile='titanic_fir_interactions.xlsx')\n",
    "pd.read_excel('titanic_fir_interactions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like pclass has a non-monotonic effect\n",
    "pd.read_excel('titanic_fir_interactions.xlsx', sheet_name='Interaction Depth 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('titanic_fir_interactions.xlsx', sheet_name='Interaction Depth 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "XGBoost is very powerful. Combining with other tools will take you a long way.\n",
    "\n",
    "Explore your data and your results.\n",
    "\n",
    "Lots of libraries. Some are better integrated.\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "* Pandas skills come in useful for manipulating data\n",
    "* Make sure you discuss business value with stake holders\n",
    "\n",
    "\n",
    "Questions?\n",
    "\n",
    "\n",
    "Connect on LinkedIn or Twitter `@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
